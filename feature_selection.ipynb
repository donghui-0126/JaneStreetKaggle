{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_dict = {}\n",
    "threshold = 0.4 # 상관관계 임계값 설정 (원하는 값으로 수정 가능)\n",
    "\n",
    "for partition_num in range(0,10):\n",
    "    file_path = f\"feature_corr/corr_csv/{partition_num}.csv\"\n",
    "    corr = pd.read_csv(file_path, index_col=0)\n",
    "    \n",
    "    # 상관관계가 threshold보다 높은 경우만 처리\n",
    "    high_corr = corr[abs(corr['correlation']) > threshold]  # correlation 열의 절댓값이 threshold보다 큰 행만 필터링\n",
    "    \n",
    "    for pair in (high_corr['var1'] + \"-\" + high_corr['var2']).values:\n",
    "        if pair in high_corr_dict.keys():\n",
    "            high_corr_dict[pair] += 1\n",
    "        else:\n",
    "            high_corr_dict[pair] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_df = pd.DataFrame([high_corr_dict], index=[\"count\"]).T\n",
    "high_corr_df.sort_values(by=\"count\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 몇개의 partition에서 threshold를 넘긴 feature 쌍이 존재하는 지 확인\n",
    "high_corr_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from community.community_louvain import best_partition  # 변경된 import 방식\n",
    "\n",
    "# 상관관계가 높은 feature pair 데이터를 네트워크 그래프로 변환\n",
    "def create_feature_clusters(df_high_corr):\n",
    "    # 네트워크 그래프 생성\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # DataFrame의 인덱스(feature pair)를 순회하며 엣지 추가\n",
    "    for pair in df_high_corr.index:\n",
    "        feat1, feat2 = pair.split('-')\n",
    "        # 가중치는 해당 pair가 발견된 횟수(count)\n",
    "        weight = df_high_corr.loc[pair, 'count']\n",
    "        G.add_edge(feat1, feat2, weight=weight)\n",
    "    \n",
    "    # Louvain 방법을 사용하여 커뮤니티(클러스터) 탐지\n",
    "    clusters = best_partition(G)\n",
    "    \n",
    "\n",
    "    \n",
    "    # 클러스터별 feature 목록 반환\n",
    "    cluster_groups = {}\n",
    "    for node, cluster_id in clusters.items():\n",
    "        if cluster_id not in cluster_groups:\n",
    "            cluster_groups[cluster_id] = []\n",
    "        cluster_groups[cluster_id].append(node)\n",
    "    \n",
    "    return cluster_groups, G\n",
    "\n",
    "# 클러스터링 실행\n",
    "cluster_groups, graph = create_feature_clusters(high_corr_df[high_corr_df['count']==10])\n",
    "\n",
    "# 클러스터 결과 출력\n",
    "for cluster_id, features in cluster_groups.items():\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    print(\", \".join(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representative_features():\n",
    "   \"\"\"\n",
    "   각 클러스터에서 가장 작은 feature 번호를 대표값으로 반환하고\n",
    "   제거할 feature 목록을 반환하는 하드코딩된 함수\n",
    "   \"\"\"\n",
    "   # 대표 feature 목록 (각 클러스터에서 가장 작은 번호)\n",
    "   representative_features = [\n",
    "       'feature_73',  # Cluster 0 \n",
    "       'feature_12',  # Cluster 1\n",
    "       'feature_15',  # Cluster 2\n",
    "       'feature_05',  # Cluster 3 \n",
    "       'feature_32',  # Cluster 5\n",
    "       'feature_42',  # Cluster 6\n",
    "       'feature_39',  # Cluster 7\n",
    "       'feature_24',  # Cluster 8\n",
    "       'feature_50',  # Cluster 4\n",
    "       'feature_09',  # Cluster 11\n",
    "       'feature_22',  # Cluster 12\n",
    "       'responder_4', # Cluster 9\n",
    "       'feature_48'   # Cluster 10\n",
    "   ]\n",
    "\n",
    "   # 제거할 feature 목록\n",
    "   features_to_remove = [\n",
    "       # Cluster 0\n",
    "       'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78',\n",
    "       \n",
    "       # Cluster 1\n",
    "       'feature_13', 'feature_14', 'feature_67', 'feature_68', 'feature_69', \n",
    "       'feature_70', 'feature_71', 'feature_72',\n",
    "       \n",
    "       # Cluster 2\n",
    "       'feature_16', 'feature_17',\n",
    "       \n",
    "       # Cluster 3\n",
    "       'feature_06', 'feature_07', 'feature_08', 'feature_18', 'feature_19',\n",
    "       'feature_37', 'feature_38', 'feature_45', 'feature_46', 'feature_56',\n",
    "       'feature_57', 'feature_58', 'feature_65', 'feature_66',\n",
    "       \n",
    "       # Cluster 5\n",
    "       'feature_34', 'feature_35', 'feature_61',\n",
    "       \n",
    "       # Cluster 6\n",
    "       'feature_44',\n",
    "       \n",
    "       # Cluster 7\n",
    "       'feature_41',\n",
    "       \n",
    "       # Cluster 8\n",
    "       'feature_25',\n",
    "       \n",
    "       # Cluster 4\n",
    "       'feature_52', 'feature_53', 'feature_55', 'feature_59', 'feature_60',\n",
    "       \n",
    "       # Cluster 11\n",
    "       'feature_11',\n",
    "       \n",
    "       # Cluster 12\n",
    "       'feature_23',\n",
    "       \n",
    "       # Cluster 9\n",
    "       'responder_7',\n",
    "       \n",
    "       # Cluster 10\n",
    "       'feature_49'\n",
    "   ]\n",
    "   \n",
    "   return representative_features, features_to_remove\n",
    "\n",
    "# 함수 사용 예시\n",
    "rep_features, remove_features = get_representative_features()\n",
    "\n",
    "print(\"대표 Features:\")\n",
    "print(sorted(rep_features))\n",
    "print(\"\\n제거할 Features:\")\n",
    "print(sorted(remove_features))\n",
    "print(f\"\\n대표 Features 개수: {len(rep_features)}\")\n",
    "print(f\"제거할 Features 개수: {len(remove_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('jane-street-real-time-market-data-forecasting/train.parquet/partition_id=6/part-0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = df.drop(remove_features, axis=1) # 대표 feature에 해당하지 않고 클러스터에 속한 feature drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_col = result_df.columns[result_df.columns.str.contains(\"feature\") | result_df.columns.str.contains(\"id\")] # 데이터 프레임에서 feature와 symbol, time, date 필터링\n",
    "responder_col = result_df.columns[result_df.columns.str.contains(\"responder\")] # target column 필터링\n",
    "rolling_feature_col = [col for col in feature_col if col not in (['date_id', 'time_id', 'symbol_id']+responder_col.to_list())] # moving average를 구할 대표 feature만 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# of feature_col:\", feature_col.shape[0])\n",
    "print(\"# of rolling_feature_col:\", len(rolling_feature_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "아래함수는 Moving Average를 생성하는 함수입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_feature(data, rolling_window_size:list, rolling_feature_col:list, symbols) -> pd.DataFrame:    \n",
    "    \"\"\"\n",
    "    data를 인풋으로 받아서 각 symbol 별로 rolling_window_size에 해당하는 window size를 갖는 Moving Average를 계산하는 함수입니다. \n",
    "    \n",
    "    data: 데이터가 담긴 DataFrame\n",
    "    rolling_window_size: Moving Average를 계산할 window size  \n",
    "    rolling_faeture_col: Moving Average를 계산할 Feautre (그냥 Data에 넘기는 방법으로 코드를 이쁘게 작성할 수 있을 듯 합니다)\n",
    "    symbols: 데이터에 존재하는 symbol (이부분도 그냥 data에 접근해서 얻을 수 있는 부분입니다 | 다만, 메모리 때문에 이렇게 구현했습니다.)\n",
    "    \"\"\"\n",
    "    # 결과를 저장할 빈 리스트\n",
    "    result_chunks = []\n",
    "    \n",
    "    # 심볼별로 순차 처리\n",
    "    for symbol in symbols:\n",
    "        print(f\"Complete Symbol {symbol}\")\n",
    "        # 1. 필요한 컬럼만 선택하여 메모리 사용 최소화\n",
    "        symbol_data = data.loc[data['symbol_id'] == symbol,].copy().ffill().bfill()\n",
    "        \n",
    "        # 2. 각 window size별로 처리\n",
    "        for window in rolling_window_size:\n",
    "            # 새로운 컬럼명\n",
    "            new_cols = [f'{col}_rolling_{window}' for col in rolling_feature_col]\n",
    "            \n",
    "            # 3. 한 번에 하나의 feature만 처리\n",
    "            for feat, new_col in zip(rolling_feature_col, new_cols):\n",
    "                symbol_data[new_col] = (symbol_data[feat]\n",
    "                                      .rolling(window=window)\n",
    "                                      .mean()\n",
    "                                      .ffill()\n",
    "                                      .bfill())\n",
    "        \n",
    "        # 4. 처리된 청크 저장\n",
    "        result_chunks.append(symbol_data)\n",
    "        \n",
    "        # 5. 메모리에서 불필요한 데이터 명시적 제거\n",
    "        del symbol_data\n",
    "    \n",
    "    # 6. 청크 병합 및 반환\n",
    "    return pd.concat(result_chunks, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_feature_slice_concat_with_save(data, rolling_window_size, rolling_feature_col, symbols_num=5, save_path='temp_results'):\n",
    "    \"\"\"\n",
    "    바로 위에 있는 함수랑 기능은 똑같은데 메모리 때문에 symbol을 5개씩 처리하는 함수입니다. \n",
    "    추가적으로 이함수는 save까지 진행합니다. \n",
    "    없애도 될 것 같아요. \n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    import gc\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # 임시 저장 디렉토리 생성\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    symbol_iter = data['symbol_id'].unique().shape[0] // symbols_num\n",
    "    \n",
    "    for i in range(symbol_iter):\n",
    "        print(f\"\\nProcessing iteration {i+1}/{symbol_iter}\")\n",
    "        \n",
    "        symbols = data.symbol_id.unique()[i*symbols_num:(i+1)*symbols_num]\n",
    "        \n",
    "        result = create_rolling_feature(\n",
    "            data=data,\n",
    "            rolling_window_size=rolling_window_size,\n",
    "            rolling_feature_col=rolling_feature_col,\n",
    "            symbols=symbols\n",
    "        )\n",
    "        \n",
    "        # 중간 결과 저장\n",
    "        result.to_parquet(f'{save_path}/chunk_{i}.parquet')\n",
    "        del result\n",
    "        gc.collect()\n",
    "    \n",
    "    # 마지막 남은 심볼들 처리\n",
    "    if data.symbol_id.unique().shape[0] % symbols_num != 0:\n",
    "        remaining_symbols = data.symbol_id.unique()[symbol_iter*symbols_num:]\n",
    "        result = create_rolling_feature(\n",
    "            data=data,\n",
    "            rolling_window_size=rolling_window_size,\n",
    "            rolling_feature_col=rolling_feature_col,\n",
    "            symbols=remaining_symbols\n",
    "        )\n",
    "        result.to_parquet(f'{save_path}/chunk_final.parquet')\n",
    "    \n",
    "\n",
    "    \n",
    "    return \n",
    "\n",
    "rolling_windows = [2, 10, 30, 50, 200, 500, 1000]\n",
    "final_df = create_rolling_feature_slice_concat_with_save(\n",
    "    data=result_df.reset_index(),\n",
    "    rolling_window_size=rolling_windows,\n",
    "    rolling_feature_col=rolling_feature_col,\n",
    "    symbols_num=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "아래 함수들은 제가 메모리에 한번에 올리지 못해서 사용한 함수입니다. <br>\n",
    "쪼개져서 저장된 데이터를 합치는 역할을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_results_optimized(file_path, save_name='final_result.parquet'):\n",
    "    \"\"\"\n",
    "    Polars를 사용해 parquet 파일들을 메모리 효율적으로 합치고 정렬하는 함수\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        parquet 파일들이 저장된 경로\n",
    "    save_name : str\n",
    "        저장할 파일명\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame: 정렬된 최종 데이터프레임\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import polars as pl\n",
    "    import gc\n",
    "    \n",
    "    print(\"Starting concatenation...\")\n",
    "    \n",
    "    # 해당 경로의 모든 parquet 파일 찾기\n",
    "    all_files = [os.path.join(file_path, f) for f in os.listdir(file_path) \n",
    "                if f.endswith('.parquet')]\n",
    "    print(f\"Found {len(all_files)} parquet files\")\n",
    "    \n",
    "    # LazyFrame으로 모든 파일 읽기\n",
    "    lfs = [pl.scan_parquet(file) for file in all_files]\n",
    "    \n",
    "    print(\"\\nConcatenating files...\")\n",
    "    # LazyFrame 상태에서 합치기\n",
    "    final_lazy = pl.concat(lfs)\n",
    "    \n",
    "    # 정렬 및 계산 실행\n",
    "    print(\"Sorting and computing...\")\n",
    "    final_df = (final_lazy\n",
    "                .sort(by='index')  # 인덱스로 정렬\n",
    "                .collect(streaming=True)  # 스트리밍 모드로 계산\n",
    "    )\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "final_df = concat_results_optimized(\n",
    "    file_path='temp_results',\n",
    "    save_name='final_rolling_features.parquet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "def save_chunks_to_directory(df: pl.DataFrame, output_dir: str, chunk_size: int = 100_000, prefix: str = \"chunk_\"):\n",
    "    \"\"\"\n",
    "    DataFrame의 각 청크를 별도의 parquet 파일로 저장\n",
    "    \n",
    "    Args:\n",
    "        df: 저장할 Polars DataFrame\n",
    "        output_dir: 저장할 디렉토리 경로\n",
    "        chunk_size: 각 청크의 크기\n",
    "        prefix: 파일명 접두사\n",
    "    \"\"\"\n",
    "    # 디렉토리가 없으면 생성\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    total_rows = df.height\n",
    "    chunks_count = (total_rows + chunk_size - 1) // chunk_size  # 올림 나눗셈\n",
    "    \n",
    "    # 각 청크를 개별 파일로 저장\n",
    "    for i in range(0, total_rows, chunk_size):\n",
    "        chunk = df.slice(i, min(i + chunk_size, total_rows))\n",
    "        chunk_number = i // chunk_size\n",
    "        file_name = f\"{prefix}{chunk_number:04d}.parquet\"  # 예: chunk_0000.parquet\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        \n",
    "        chunk.write_parquet(\n",
    "            file_path,\n",
    "            compression=\"snappy\",\n",
    "            statistics=False,\n",
    "            use_pyarrow=True\n",
    "        )\n",
    "        \n",
    "        # 진행상황 출력 (선택사항)\n",
    "        print(f\"저장 완료: {file_name} ({chunk_number + 1}/{chunks_count})\")\n",
    "\n",
    "# 사용 예시\n",
    "# final_df를 chunks 디렉토리에 저장\n",
    "save_chunks_to_directory(\n",
    "    df=final_df, \n",
    "    output_dir=\"chunks\", \n",
    "    chunk_size=1_000_000,\n",
    "    prefix=\"final_rolling_\"\n",
    ")\n",
    "\n",
    "# 나중에 모든 청크를 다시 읽을 때 사용할 수 있는 함수\n",
    "def read_all_chunks(directory: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    디렉토리의 모든 parquet 파일을 읽어서 하나의 DataFrame으로 결합\n",
    "    \"\"\"\n",
    "    # parquet 파일 목록 가져오기\n",
    "    parquet_files = sorted([\n",
    "        os.path.join(directory, f) \n",
    "        for f in os.listdir(directory) \n",
    "        if f.endswith('.parquet')\n",
    "    ])\n",
    "    \n",
    "    # 모든 파일 읽어서 결합\n",
    "    return pl.concat([\n",
    "        pl.read_parquet(f) \n",
    "        for f in parquet_files\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jane_street",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
